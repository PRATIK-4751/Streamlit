ğŸŒŒ NeonCoder AI ğŸŒŒ
Welcome to NeonCoder AI, a cutting-edge, neon-themed coding assistant built with Streamlit and powered by the Qwen2.5-coder model via Ollama. This tool empowers developers to generate Python code, visualize data with stunning graphs, and deploy apps across platforms like Docker, FastAPI, Streamlit, and AWS Lambdaâ€”all wrapped in a futuristic cyber neon interface! ğŸš€
Made by Pratik (PRATIK-4751 on GitHub)Last Updated: August 04, 2025, 05:23 AM IST

âœ¨ Features âœ¨



Icon
Feature
Description



ğŸ’»
Code Wizard
Generate Python code (e.g., Fibonacci sequence, data analysis) with one click.


ğŸ“Š
Graph Execution
Toggle interactive Plotly charts to visualize code output.


ğŸš€
Deploy Master
Create deployment configs for Docker, FastAPI, Streamlit, or AWS Lambda.


ğŸ¨
Neon UI
Sleek, customizable cyber neon design with glowing green accents.


â±ï¸
Real-Time Feedback
Test your Ollama server connection from the sidebar.


ğŸ”„
User-Friendly Tabs
Seamlessly switch between coding and deployment tasks.



ğŸ› ï¸ Prerequisites ğŸ› ï¸
Before launching NeonCoder AI, ensure these are installed:



Requirement
Details



ğŸ Python 3.8+
Required runtime environment.


ğŸ“‚ Git
For cloning the repository.


ğŸ¤– Ollama
Local AI model server with qwen2.5-coder:7b.


ğŸ“¦ Python Libraries
streamlit, requests, pandas, plotly-express.



ğŸ“¥ Installation ğŸ“¥
Step 1: Clone the Repository
# Navigate to your desired directory
cd ~/Desktop

# Clone the repo
git clone https://github.com/PRATIK-4751/Streamlit_coder-Deployer.git

# Enter the project directory
cd Streamlit_coder-Deployer

Step 2: Set Up a Virtual Environment
# Create a virtual environment
python -m venv venv

# Activate it
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

Step 3: Install Dependencies
# Install required packages
pip install -r requirements.txt

If requirements.txt is missing, create it with:
streamlit
requests
pandas
plotly-express

Step 4: Set Up Ollama
# Install Ollama from https://ollama.ai/
# Pull the model
ollama pull qwen2.5-coder:7b

# Start the server
ollama serve

Ensure it runs on http://localhost:11434 (default port).

â–¶ï¸ Running the Application â–¶ï¸

With the virtual environment active and Ollama running, start the app:
streamlit run app.py

(Replace app.py with your script name if different.)

Open your browser at the URL Streamlit provides (usually http://localhost:8501).

Test the connection in the sidebar by clicking "Test Connection". A âœ… ğŸŸ¢ Ollama is running indicates success!



ğŸ® Usage Instructions ğŸ®
ğŸ’» Code Wizard Tab



Action
Instruction



ğŸ“ Describe Task
Enter a task (e.g., "Create a Fibonacci function") in the text area.


ğŸ”§ Generate Code
Click "Generate Code" to get Python code.


ğŸ“ˆ Graph Toggle
Enable "Enable Graph Execution" for Plotly charts if supported.


ğŸ‘ï¸ Output
View generated code and graphs in the UI.


ğŸš€ Deploy Master Tab



Action
Instruction



ğŸŒ Select Platform
Choose Docker, FastAPI, Streamlit, or AWS Lambda.


ğŸ“ Describe Deployment
Enter details (e.g., "Docker for a web app").


ğŸ› ï¸ Build Deployment
Click "Build Deployment" for scripts/configs.



ğŸ¨ Customization ğŸ¨

Neon UI: Tweak the CSS in the st.markdown() style block to adjust colors or effects.
Model Config: Modify OLLAMA_HOST or MODEL variables for different servers/models.


âš ï¸ Troubleshooting âš ï¸



Issue
Solution



ğŸ”´ Ollama Error
Ensure server is running at http://localhost:11434; check firewall.


ğŸ“¦ Module Missing
Run `pip list


ğŸ“‰ Graph Failure
Ensure code uses compatible data (e.g., Pandas DataFrames).



ğŸ¤ Contributing ğŸ¤
Fork this repo, enhance it, and submit pull requests! Collaborate via GitHub or add contributors in the repo settings.

ğŸ“œ License ğŸ“œ
Open-source project. Use and modify freely, but credit Pratik (PRATIK-4751).

ğŸ™Œ Acknowledgments ğŸ™Œ

Built with â¤ï¸ by Pratik.
Powered by Streamlit, Ollama, and the Qwen2.5-coder model.
Gratitude to the open-source community! ğŸŒ
